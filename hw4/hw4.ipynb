{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "#### Part 1 (35 points): Implement your RNN either using an existing framework OR you can implement your own RNN cell structure. In either case, describe the structure of your RNN and the activation functions you are using for each time step and in the output layer. Define a metric you will use to measure the performance of your model (NOTE: Performance should be measured both for the validation set and the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "import keras \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB sentiment classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. \n",
    "\n",
    "IMDB provided a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. \n",
    "\n",
    "There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. \n",
    "\n",
    "reference: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Why RNN for sentiment classification? RNN models are mostly used in the fields of natural language processing, dealing with sequential data. In sentiment classification for IMDB dataset, we are dealing with sequential input for outputing either positive or negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (24000, 100)\n",
      "X_val shape: (1000, 100)\n",
      "X_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "#Load data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "num_validation = 1000\n",
    "X_val = X_train[-num_validation:]\n",
    "y_val = y_train[-num_validation:]\n",
    "X_train = X_train[:num_training]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN  \n",
    "https://keras.io/api/layers/recurrent_layers/simple_rnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, \n",
    "                                 kernel_initializer='glorot_uniform', \n",
    "                                 recurrent_initializer='orthogonal', \n",
    "                                 bias_initializer='zeros', \n",
    "                                 kernel_regularizer=None, \n",
    "                                 recurrent_regularizer=None, \n",
    "                                 bias_regularizer=None, \n",
    "                                 activity_regularizer=None, \n",
    "                                 kernel_constraint=None, recurrent_constraint=None, \n",
    "                                 bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<tr>\n",
    "<td> <img src=\"./rnn-many-to-one-ltr.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"./description-block-rnn-ltr.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{<t>}=g_{1}\\left(W_{a a} a^{<t-1>}+W_{a x} x^{<t>}+b_{a}\\right) \\quad \\text { and } y=g_{2}\\left(W_{y a} a^{<t>}+b_{y}\\right)$ where $W_{a x}, W_{a a}, W_{y a}, b_{a}, b_{y}$ are coefficients that are shared temporally and \n",
    "$g_1, g_2$ are activation functions. Actication functions here are tanh, where $$g(z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$\n",
    "\n",
    "\n",
    "<a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\">https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 27s 36ms/step - loss: 0.5907 - val_loss: 0.5112\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 0.4263 - val_loss: 0.6014\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 0.3870 - val_loss: 0.5006\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 0.3576 - val_loss: 0.5022\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 25s 34ms/step - loss: 0.2960 - val_loss: 0.5672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a436faf0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(SimpleRNN(128))  \n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, \n",
    "          epochs=5, validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The metric we use for classification is accuracy, which is defined as the ratio of correct prediction over total predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.9166666666666666\n",
      "val accuracy: 0.761\n",
      "Test accuracy: 0.77408\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_train, y_pred))\n",
    "print('training accuracy:', acc)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_val, y_pred))\n",
    "print('val accuracy:', acc)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_test, y_pred))\n",
    "# print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (25 points): Update your network from part 1 with either an LSTM or a GRU based cell structure. Re-do the training and performance evaluation. What are the major differences you notice? Why do you think those differences exist between the 2 implementations?\n",
    "\n",
    "### LSTM  \n",
    "https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "<img src=\"./lstm-ltr.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "$$\\Gamma=\\sigma\\left(W x^{<t>}+U a^{<t-1>}+b\\right)$$ where $W,U,b$ are coefficients specific to the gate and $\\sigma$ is the sigmoid function. \n",
    "$$\\tilde{c}^{<t>} = \\tanh \\left(W_{c}\\left[\\Gamma_{r} \\cdot a^{<t-1>}, x^{<t>}\\right]+b_{c}\\right)$$\n",
    "$${c}^{<t>} = \\Gamma_{u} \\cdot \\tilde{c}^{<t>}+\\Gamma_{f} \\cdot c^{<t-1>}$$\n",
    "$${a}^{<t>} = \\Gamma_{o} \\cdot c^{<t>}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
    "                            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "                            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "                            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "                            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, \n",
    "                            dropout=0.0, recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/5\n",
      "750/750 [==============================] - 46s 61ms/step - loss: 0.4220 - val_loss: 0.3502\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 45s 61ms/step - loss: 0.2354 - val_loss: 0.3778\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 47s 62ms/step - loss: 0.1453 - val_loss: 0.4282\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 48s 64ms/step - loss: 0.0916 - val_loss: 0.5002\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 50s 66ms/step - loss: 0.0610 - val_loss: 0.6615\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6973\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen)) \n",
    "model.add(LSTM(128))  \n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, \n",
    "          epochs=5, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.9856666666666667\n",
      "val accuracy: 0.822\n",
      "Test accuracy: 0.82424\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_train, y_pred))\n",
    "print('training accuracy:', acc)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_val, y_pred))\n",
    "print('val accuracy:', acc)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_test, y_pred))\n",
    "# print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU  \n",
    "https://keras.io/api/layers/recurrent_layers/gru/\n",
    "<img src=\"./gru-ltr.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "$$\\tilde{c}^{<t>} = \\tanh \\left(W_{c}\\left[\\Gamma_{r} \\cdot a^{<t-1>}, x^{<t>}\\right]+b_{c}\\right)$$\n",
    "$${c}^{<t>} = \\Gamma_{u} \\cdot \\tilde{c}^{<t>}+\\left(1-\\Gamma_{u}\\right) \\cdot c^{<t-1>}$$\n",
    "$${a}^{<t>} = c^{<t>}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "keras.layers.recurrent.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
    "                           kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "                           bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, \n",
    "                           bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "                           recurrent_constraint=None, bias_constraint=None, \n",
    "                           dropout=0.0, recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 42s 54ms/step - loss: 0.4581 - val_loss: 0.3784\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 41s 54ms/step - loss: 0.2468 - val_loss: 0.4013\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 42s 57ms/step - loss: 0.1414 - val_loss: 0.4122\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 42s 56ms/step - loss: 0.0735 - val_loss: 0.5356\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 42s 56ms/step - loss: 0.0388 - val_loss: 0.6831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29cd0a9d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(GRU(128))  \n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, \n",
    "          epochs=5, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.9958333333333333\n",
      "val accuracy: 0.833\n",
      "Test accuracy: 0.83404\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_train, y_pred))\n",
    "print('training accuracy:', acc)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_val, y_pred))\n",
    "print('val accuracy:', acc)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "y_pred = y_pred.flatten()\n",
    "acc = np.mean(np.equal(y_test, y_pred))\n",
    "# print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For LSTM and GRU, we clearly see a faster convergence for the training loss and improved prediction accuracy for test and validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "In this task, use any of the pre-trained word embeddings. The Wor2vec embedding link provided with the lecture notes can be useful to get started. Write your own code/function that uses these embeddings and outputs cosine similarity and a dissimilarity score for any 2 pair of words (read as user input). The dissimilarity score should be defined by you. You either can have your own idea of a dissimilarity score or refer to literature (cite the paper you used). In either case clearly describe how this score helps determine the dissimilarity between 2 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/ulysses/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 words similar to car\n",
      "('truck', 0.7733270525932312)\n",
      "('boat', 0.7584270238876343)\n",
      "('wagon', 0.7152294516563416)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD4CAYAAAAeugY9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfO0lEQVR4nO3deXhW5Z3/8feXiCSAgA6BsilppwSBELIQUZSqWMEBI7iMVXSki9QFRh3LFNpeLf7USypMtXTqkqmMWmmxIsaCVZAKVVkkCQmGVQTCYECMUiJgAlm+vz8SnrIkEMiTPMnJ53Vdua48Z7nP98T44c597nOOuTsiItK8tYp0ASIiUn8KcxGRAFCYi4gEgMJcRCQAFOYiIgFwViQO2rlzZ+/du3ckDi0i0mzl5OR87u6xNa2LSJj37t2b7OzsSBxaRKTZMrMdta3TMIuISAAozEVEAkBhHkDl5eWRLkFEGllExsylbh5++GFeeuklYmNj6dWrFykpKYwdO5Z7772XoqIi2rZty//8z//Qt29fxo8fT3R0NLm5uQwdOpS9e/cSExNDbm4un332GbNnz+bFF19k5cqVXHTRRTz//PMA3H333WRlZVFSUsKNN97IQw89BFRd17jjjjtYsGABZWVlvPLKK/Tp04f4+HhWrFhBbGwslZWV9OnTh5UrVxIbW+M1GRFpJOqZN1FZWVm8+uqrrF27ljfffDN0wXjChAn85je/IScnh5kzZ3LPPfeE9vnkk09YsWIFv/rVrwD4+9//zsqVK3niiSdIT0/ngQceYP369eTn55OXlwfAo48+SnZ2Nh9++CF/+9vf+PDDD0Ptde7cmTVr1nD33Xczc+ZMWrVqxW233cacOXMAWLJkCYmJiQpykSZAPfMmJDO3kBmLNrNrXwms+wtpaVcQHR1NdHQ01157LaWlpaxYsYKbbroptM+hQ4dC3990001ERUWFPl977bWYGQkJCXTt2pWEhAQA+vfvT0FBAYMGDeJPf/oTGRkZlJeXs3v3bjZs2MDAgQMBuP766wFISUlh/vz5AHzve9/juuuu4/7772f27Nl897vfbfCfi4icWljC3Mw6Ab8DBgAOfM/dV4aj7ZYiM7eQqfPzKSmrAODLkjL+umkfmbmFjEnqAUBlZSWdOnUK9aqP165du2M+t2nTBoBWrVqFvj/yuby8nO3btzNz5kyysrI499xzGT9+PKWlpSfsHxUVFRqH79WrF127duWdd95h9erVoV66iERWuIZZfg285e59gURgY5jabTFmLNocCnKANj0vZP9HH/DLhfkcOHCAhQsX0rZtW+Li4njllVcAcHfWrl17xsf88ssvadeuHR07dmTPnj28+eabddrvBz/4AbfddtsJfwmISOTUO8zNrCMwDHgOwN0Pu/u++rbb0uzaV3LM5zbd+hDzz2lkP/F9rrnmGhISEujYsSNz5szhueeeIzExkf79+/P666+f8TETExNJSkqib9++3HrrrQwdOrRO+6Wnp3PgwAENsYg0IVbfl1OY2SAgA9hAVa88B7jP3Q8et90EYALA+eefn7JjR603MrVIQ6e/Q+FxgV55uIReXc7j7X8fwrBhw8jIyCA5OTlCFf5DdnY2DzzwAO+9916kSxFpUcwsx91Ta1oXjmGWs4Bk4Gl3TwIOAlOO38jdM9w91d1TNfvhRJNHxBPT+tghi+LFv2XX/04iOTmZG264oUkE+fTp07nhhht47LHHIl2KiBwlHD3zrwGr3L139efLgCnuPqq2fVJTU13PZjnR0bNZuneKYfKI+NDFTxGRk/XM6z2bxd0/NbOdZhbv7puB4VQNuchpGpPUQ+EtImckXPPMJwFzzOxsYBugK2MiIo0oLGHu7nlAjV1/ERFpeLqdX0QkABTmIiIBoDAXEQkAhbmISAAozEVEAkBhLiISAApzEZEAUJiLiASAwlxEJAAU5iIiAaAwFxEJAIW5iEgAKMxFRAJAYS4iEgAKcxGRAFCYi4gEgMJcRCQAFOYiIgGgMBcRCQCFuYhIACjMRUQCQGEuIhIACnMRkQBQmIuIBIDCXEQkAMIW5mYWZWa5ZrYwXG2KiEjdhLNnfh+wMYztBcasWbO48MILGTduXKRLEZGAOiscjZhZT2AU8CjwH+FoM0ieeuoplixZQs+ePUPLysvLOeussPz4RUTC1jN/EvhPoLK2Dcxsgpllm1l2UVFRmA7b9N11111s27aNa665ho4dO3L77bczdOhQbr/9dgoKCrjssstITk4mOTmZFStWALBs2TIuv/xybrzxRvr27cu4ceNwdwCysrK45JJLSExMJC0tjf3791NRUcHkyZMZPHgwAwcO5Nlnn43kKYtIJLh7vb6A0cBT1d9fDiw81T4pKSneklxwwQVeVFTkv/jFLzw5Odm/+uord3c/ePCgl5SUuLv7Rx995Ed+LkuXLvUOHTr4zp07vaKiwocMGeLvvfeeHzp0yOPi4nz16tXu7l5cXOxlZWX+7LPP+sMPP+zu7qWlpZ6SkuLbtm2LwJmKSEMCsr2WXA3H3/lDgXQz+xcgGuhgZi+5+21haLvZyswtZMaizezaV8KnxaX85cPdAKSnpxMTEwNAWVkZEydOJC8vj6ioKD766KPQ/mlpaaFhmUGDBlFQUEDHjh3p1q0bgwcPBqBDhw4ALF68mA8//JB58+YBUFxczJYtW4iLi2u08xWRyKp3mLv7VGAqgJldDvxIQV7I1Pn5lJRVAFBe6Tz8xgZSDnxJ6je7h7Z74okn6Nq1K2vXrqWyspLo6OjQujZt2oS+j4qKory8vNbjuTu/+c1vGDFiRAOcjYg0B5pn3gBmLNocCvIjSssqWL71i2OWFRcX061bN1q1asXvf/97KiqO3ed48fHx7N69m6ysLAD2799PeXk5I0aM4Omnn6asrAyAjz76iIMHD4bxjESkqQvrdAp3XwYsC2ebzdGufSU1Lt9fWnbM53vuuYcbbriBF198kZEjR9KuXbuTtnv22Wfz8ssvM2nSJEpKSoiJiWHJkiX84Ac/oKCggOTkZNyd2NhYMjMzw3U6ItIMmFfPkmhMqampnp2d3ejHbSxDp79DYQ2B3qNTDMunXBmBikQkCMwsx91Ta1qnYZYGMHlEPDGto45ZFtM6iskj4iNUkYgEncK8AYxJ6sFj1yfQo1MMRlWP/LHrExiT1OO02rnkkktOa/tly5YxevRoAP785z8zffr009pfRJov3YLYQMYk9Tjt8D7ekZuIzkR6ejrp6en1Or6INB/qmTdh7du3B05+R+hbb71F3759SU5OZv78+aF9n3/+eSZOnAjAggULuOiii0hKSuKqq65iz549jX8yItKgFObNRG5uLk8++SQbNmxg27ZtLF++nNLSUu68804WLFhATk4On376aY37XnrppaxatYrc3Fy+853v8Pjjjzdy9SLS0DTM0kzUdEdo+/btiYuL45vf/CYAt912GxkZGSfs+8knn3DzzTeze/duDh8+rDtDRQJIPfMmJjO3kKHT3yFuyhuUlFWQmVsInN4docebNGkSEydOJD8/n2effZbS0tKw1y0ikaUwb0KOPAagcF8JDrjD1Pn5vL+l5qdM9u3bl4KCArZu3QrAH//4xxq3Ky4upkePqouxL7zwQoPULiKRpTBvQmp6DEBJWQVzs3bWuH10dDQZGRmMGjWK5ORkunTpUuN206ZN46abbiIlJYXOnTuHvW4RiTzdAdqExE15g5r+axiwffqoxi5HRJoY3QHaTHTvFHNay0VEjlCYNyF6DICInClNTWxCjtwxeuSlFt07xTB5RHy97yQVkeBTmDcx4XgMgIi0PBpmEREJAIW5iEgAKMxFRAJAYS4iEgAKcxGRAFCYi4gEgMJcRCQAFOYiIgGgMBcRCQCFuYhIANQ7zM2sl5ktNbMNZrbezO4LR2EiIlJ34Xg2SznwoLuvMbNzgBwze9vdN4ShbRERqYN698zdfbe7r6n+fj+wEdCTokREGlFYx8zNrDeQBHxQw7oJZpZtZtlFRTW/01JERM5M2MLczNoDrwL3u/uXx6939wx3T3X31NjY2HAdVkRECFOYm1lrqoJ8jrvPD0ebIiJSd+GYzWLAc8BGd/9V/UsSEZHTFY6e+VDgduBKM8ur/vqXMLQrIiJ1VO+pie7+PmBhqEVERM6Q7gAVEQmDadOmMXPmzIgdX2EuIgK4O5WVlZEu44wpzEWkxSooKCA+Pp5/+7d/Y8CAAXz/+99nwIABJCQk8PLLLwNw4MABhg8fTnJyMgkJCbz++uuh/R999FH69OnDpZdeyubNmyN1GkB4bucXEWm2tmzZwgsvvEBhYSHPPPMMa9eu5fPPP2fw4MEMGzaM2NhYXnvtNTp06MDnn3/OkCFDSE9PZ82aNcydO5e8vDzKy8tJTk4mJSUlYuehMBeRFiUzt5AZizaza18J53kxsd16MmTIEB544AFuueUWoqKi6Nq1K9/61rfIysrimmuu4Sc/+QnvvvsurVq1orCwkD179vDee+8xduxY2rZtC0B6enpEz0thLiItRmZuIVPn51NSVgHAni9L2VfWiszcwlr3mTNnDkVFReTk5NC6dWt69+5NaWlpY5VcZxozF5EWY8aizaEgP8LdmbFoM5dddhkvv/wyFRUVFBUV8e6775KWlkZxcTFdunShdevWLF26lB07dgAwbNgwMjMzKSkpYf/+/SxYsCASpxSinrmItBi79pXUunzs2LGsXLmSxMREzIzHH3+cr33ta4wbN45rr72WhIQEUlNT6du3LwDJycncfPPNJCYm0qVLFwYPHtyYp3ICc/dGP2hqaqpnZ2c3+nFFpGUbOv0dCmsI9B6dYlg+5coIVHR6zCzH3VNrWqdhFhFpMSaPiCemddQxy2JaRzF5RHyEKgofDbOISIsxJqnqvTlHZrN07xTD5BHxoeXNmcJcRFqUMUk9AhHex9Mwi4hIACjMRUQCQGEuIhIACnMRkQBQmIuIBIDCXEQkABTmIiIBoDAXEQkAhbmISAAozEVEAkBhLiISAApzEZEACEuYm9lIM9tsZh+b2ZRwtCkiInVX7zA3syjgt8A1QD/gFjPrV992RUSk7sLRM08DPnb3be5+GJgLXBeGdkVEpI7CEeY9gJ1Hff6ketkxzGyCmWWbWXZRUVEYDisiIkc02gVQd89w91R3T42NjW2sw4qItAjhCPNCoNdRn3tWLxMRkUYSjjDPAr5pZnFmdjbwHeDPYWhXRETqqN7vAHX3cjObCCwCooDZ7r6+3pWJiEidheWFzu7+F+Av4WhLREROn+4AFREJAIW5iEgAKMxFRAJAYS5yBvbt28dTTz0VlraWLVvG6NGjw9KWtFwKc5EzUFuYl5eXR6AaEYW5yBmZMmUKW7duZdCgQQwePJjLLruM9PR0+vXrR0FBAQMGDAhtO3PmTKZNmwbAxx9/zFVXXUViYiLJycls3br1mHazsrJISko6YbnIqYRlaqJISzN9+nTWrVtHXl4ey5YtY9SoUaxbt464uDgKCgpq3W/cuHFMmTKFsWPHUlpaSmVlJTt3Vj3aaMWKFUyaNInXX3+d888/v5HORIJCYS5yGjJzC5mxaDM7dhSw9/ODZOYW0glIS0sjLi7upPvu37+fwsJCxo4dC0B0dHRo3caNG5kwYQKLFy+me/fuDXgGElQaZhGpo8zcQqbOz6dwXwkA5RWVTJ2fz/tbimjXrl1ou7POOovKysrQ59LS0lO23a1bN6Kjo8nNzQ1/4dIiKMxF6mjGos2UlFUAYGfHUHm4hJKyCuZm7Txmu65du/LZZ5/xxRdfcOjQIRYuXAjAOeecQ8+ePcnMzATg0KFDfPXVVwB06tSJN954g6lTp7Js2bJGOycJDoW5SB3tqu6RA0TFdKBNj37seu4etix45pjtWrduzc9//nPS0tL49re/Td++fUPrfv/73zNr1iwGDhzIJZdcwqeffhpa17VrVxYuXMi9997LBx980PAnJIFi7t7oB01NTfXs7OxGP65IfQyd/k5oiOVoPTrFsHzKlRGoSFoaM8tx99Sa1qlnLlJHk0fEE9M66phlMa2jmDwiPkIVifyDZrOI1NGYpKq3Ic5YtJld+0ro3imGySPiQ8tFIklhLnIaxiT1UHhLk6RhFhGRAFCYi4gEgMJcRCQAFOYiIgGgMBcRCQCFuYhIACjMRUQCQGEuIhIACnMRkQBQmIuINAOner9svW7nN7MZwLXAYWAr8F1331efNkVEgu7FF19k5syZmBkDBw7kX//1X3nkkUc4fPgw//RP/8ScOXPo2rUr06ZNY+vWrWzbtu2UrxKs77NZ3gamunu5mf0SmAr8uJ5tiogE1vr163nkkUdYsWIFnTt3Zu/evZgZq1atwsz43e9+x+OPP85//dd/AbBhwwbef/99YmJimDt3bq3t1ivM3X3xUR9XATfWpz0RkaA68v7YTX/9EzE9BvP+zkOM6QznnXce+fn53HzzzezevZvDhw8f8z7Z9PR0YmJiTtl+OMfMvwe8WdtKM5tgZtlmll1UVBTGw4qING1Hvz/Wgf2Hypk6P5/M3EIAJk2axMSJE8nPz+fZZ5895r2xR79f9mROGeZmtsTM1tXwdd1R2/wUKAfm1NaOu2e4e6q7p8bGxtapOBGRIDj6/bHR5w/kq03vc+DLvzNj0Wb27t1LcXExPXpUPVr5hRdeOKNjnHKYxd2vOtl6MxsPjAaGeyTeQSci0sQd/f7Ys2MvoOPFN7PnD1PYY634j03fYtq0adx0002ce+65XHnllWzfvv20j1Gvd4Ca2UjgV8C33L3OYyd6B6iItCThen9sQ74D9L+Bc4C3zSzPzJ451Q4iIi1NY7w/tr6zWf45XIWIiARVY7w/Vu8AFRFpBA39/ljdzi8iEgAKcxGRAFCYi4gEgMJcRCQAFOYiIgGgMBcRCQCFuYhIACjMRUQCQGEuIhIACnMRkQBQmIuIBIDCXEQkABTmIiIBoDAXEQkAhbmISAAozEVEAkBhLiISAApzEZEAUJiLiASAwlxEJAAU5iIiAaAwFxEJAIW5iEgAhCXMzexBM3Mz6xyO9kRE5PTUO8zNrBdwNfB/9S9HRETORDh65k8A/wl4GNoSEZEzUK8wN7PrgEJ3XxumekRE5AycdaoNzGwJ8LUaVv0U+AlVQyynZGYTgAkA559//mmUKCIip2LuZzY6YmYJwF+Br6oX9QR2AWnu/unJ9k1NTfXs7OwzOq6ISEtlZjnunlrTulP2zGvj7vlAl6MOUgCkuvvnZ9qmiIicGc0zFxEJgDPumR/P3XuHqy0RETk96pmLiASAwlxEJAAU5iIiAaAwFxEJAIW5iEgAKMxFRAJAYS4iEgAKcxGRAFCYi4gEQGDC/Mknn+Srr7469YbHad++/Rkf8/nnn2fXrl1nvL+ISLi0+DCvD4W5iDQVzTLMDx48yKhRo0hMTGTAgAE89NBD7Nq1iyuuuIIrrrgCOLbHPW/ePMaPHw/A9u3bufjii0lISOBnP/vZMe3OmDGDwYMHM3DgQH7xi18AUFBQwIUXXsidd95J//79ufrqqykpKWHevHlkZ2czbtw4Bg0aRElJSeOcvIhIDZplmL/11lt0796dtWvXsm7dOu6//366d+/O0qVLWbp06Un3ve+++7j77rvJz8+nW7duoeWLFy9my5YtrF69mry8PHJycnj33XcB2LJlC/feey/r16+nU6dOvPrqq9x4442kpqYyZ84c8vLyiImJadBzFhE5mWYZ5gkJCbz99tv8+Mc/5r333qNjx4513nf58uXccsstANx+++2h5YsXL2bx4sUkJSWRnJzMpk2b2LJlCwBxcXEMGjQIgJSUFAoKCsJ2LiIi4RC2R+A2tMzcQmYs2syufSV07xTDw88vxD7J42c/+xnDhw8/YXszC31fWlpa67oj3J2pU6fywx/+8JjlBQUFtGnTJvQ5KipKQyoi0uQ0i555Zm4hU+fnU7ivBAd27PyERxZto33/K5g8eTJr1qzhnHPOYf/+/aF9unbtysaNG6msrOS1114LLR86dChz584FYM6cOaHlI0aMYPbs2Rw4cACAwsJCPvvss5PWdfwxRUQipVn0zGcs2kxJWUXoc1lRAdtf+V/GvRBFvx7n8vTTT7Ny5UpGjhwZGjufPn06o0ePJjY2ltTU1FBI//rXv+bWW2/ll7/8Jdddd12ozauvvpqNGzdy8cUXA1UXUF966SWioqJqrWv8+PHcddddxMTEsHLlSg4dOsQf/vAH7rnnnrCd+7Rp02jfvj0/+tGPwtamiATPGb/QuT5O94XOcVPeoKYqDdg+fVTY6qqvgoICRo8ezbp168LWZn3DvKKi4qT/IIlI83GyFzo3i2GW7p1qnilS2/JImTJlClu3bmXQoEFMnjy5xqmOAGPGjCElJYX+/fuTkZERWv7WW2+RnJxMYmLiMdcBNmzYwOWXX87Xv/51Zs2aFVr+0ksvkZaWxqBBg/jhD39IRUXVXy/t27fnwQcfJDExkZUrVzbCmYtIpDWLMJ88Ip6Y1sf2LmNaRzF5RHyEKqrZ9OnT+cY3vkFeXh7f/va3a53qOHv2bHJycsjOzmbWrFl88cUXFBUVceedd/Lqq6+ydu1aXnnllVC7mzZtYtGiRaxevZqHHnqIsrIyNm7cyMsvv8zy5cvJy8sjKioqdA3g4MGDXHTRRaxdu5ZLL700Ij8LEWlczWLMfExSD4BjZrNMHhEfWh5pR2ba7NhRwN7PD5KZW8j7R011BDhw4ABbtmxh2LBhzJo1K3RRdufOnWzZsoWioiKGDRtGXFwcAOedd16o/VGjRtGmTRvatGlDly5d2LNnD3/961/Jyclh8ODBAJSUlNClSxegasbNDTfc0Jg/AhGJsGYR5lAV6E0lvI92ZKbNkQu05RWVTJ2fT589+2uc6rhs2TKWLFnCypUradu2LZdffvkJUyePd/zUyPLyctydO+64g8cee+yE7aOjozVOLtLCNIthlqbs6Jk2dnYMlYdLKCmr4OPW36hxqmNxcTHnnnsubdu2ZdOmTaxatQqAIUOG8O6777J9+3YA9u7de9LjDh8+nHnz5oWmT+7du5cdO3Y01GmKSBPXbHrmTdWuff+4gSgqpgNtevRj13P3EPP1VP7frbeeMNVx5MiRPPPMM1x44YXEx8czZMgQAGJjY8nIyOD666+nsrKSLl268Pbbb9d63H79+vHII49w9dVXU1lZSevWrfntb3/LBRdc0LAnLCJNUrOYmtiUDZ3+DoX7TrwjtEenGJZPuTICFYlIUDXo1EQzm2Rmm8xsvZk9Xt/2mpvmMtNGRIKtXsMsZnYFcB2Q6O6HzKxLeMpqPpr6TBsRaRnqO2Z+NzDd3Q8BuPvJH2YSUE11po2ItBz1HWbpA1xmZh+Y2d/MbHBtG5rZBDPLNrPsoqKieh5WRESOdsqeuZktAb5Ww6qfVu9/HjAEGAz8ycy+7jVcVXX3DCADqi6A1qdoERE51inD3N2vqm2dmd0NzK8O79VmVgl0BtT1FhFpRPUdZskErgAwsz7A2cDn9WxTREROU30vgM4GZpvZOuAwcEdNQywiItKwInLTkJkVAc3p3vPONK+/OFRvw2tuNavehtVY9V7g7rE1rYhImDc3ZpZd211XTZHqbXjNrWbV27CaQr160JaISAAozEVEAkBhXjcZp96kSVG9Da+51ax6G1bE69WYuYhIAKhnLiISAApzEZEAUJifJjN70MzczDpHupaTMbMZ1c+Z/9DMXjOzTpGuqSZmNtLMNpvZx2Y2JdL1nIyZ9TKzpWa2ofr5/fdFuqa6MLMoM8s1s4WRrqUuzKyTmc2r/v3daGYXR7qmkzGzB6p/H9aZ2R/NLDoSdSjMT4OZ9QKuBv4v0rXUwdvAAHcfCHwETI1wPScwsyjgt8A1QD/gFjPrF9mqTqoceNDd+1H1cLl7m3i9R9wHbIx0Eafh18Bb7t4XSKQJ125mPYB/B1LdfQAQBXwnErUozE/PE8B/Ak3+qrG7L3b38uqPq4CekaynFmnAx+6+zd0PA3OpetlJk+Tuu919TfX3+6kKmSb9IHsz6wmMAn4X6Vrqwsw6AsOA5wDc/bC774toUad2FhBjZmcBbYFdkShCYV5HZnYdUOjuayNdyxn4HvBmpIuoQQ9g51GfP6GJh+MRZtYbSAI+iHApp/IkVR2QygjXUVdxVD119X+rh4Z+Z2btIl1Ubdy9EJhJ1V/ru4Fid18ciVoU5kcxsyXV417Hf10H/AT4eaRrPNop6j2yzU+pGh6YE7lKg8XM2gOvAve7+5eRrqc2ZjYa+MzdcyJdy2k4C0gGnnb3JOAg0GSvpZjZuVT9NRkHdAfamdltkailvk9NDJTant1uZglU/cdaa2ZQNWSxxszS3P3TRizxGCd71jyAmY0HRgPDm+jTLAuBXkd97lm9rMkys9ZUBfkcd58f6XpOYSiQbmb/AkQDHczsJXePSNjU0SfAJ+5+5C+eeTThMAeuAra7exGAmc0HLgFeauxC1DOvA3fPd/cu7t7b3XtT9QuXHMkgPxUzG0nVn9fp7v5VpOupRRbwTTOLM7Ozqbpw9OcI11Qrq/qX/Dlgo7v/KtL1nIq7T3X3ntW/s98B3mniQU71/1M7zSy+etFwYEMESzqV/wOGmFnb6t+P4UTogq165sH130Ab4O3qvyZWuftdkS3pWO5ebmYTgUVUzQKY7e7rI1zWyQwFbgfyzSyvetlP3P0vkSspkCYBc6r/gd8GfDfC9dTK3T8ws3nAGqqGM3OJ0K39up1fRCQANMwiIhIACnMRkQBQmIuIBIDCXEQkABTmIiIBoDAXEQkAhbmISAD8fxQgqLOHhKJvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Vanilla example for word2vec embedding\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    " \n",
    "nltk.download(\"brown\")\n",
    " \n",
    "# Preprocessing data to lowercase all words and remove single punctuation words\n",
    "document = brown.sents()\n",
    "data = []\n",
    "for sent in document:\n",
    "    new_sent = []\n",
    "    for word in sent:\n",
    "        new_word = word.lower()\n",
    "        if new_word[0] not in string.punctuation:\n",
    "            new_sent.append(new_word)\n",
    "    if len(new_sent) > 0:\n",
    "        data.append(new_sent)\n",
    "        \n",
    "# Creating Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences = data,\n",
    "    vector_size = 50,\n",
    "    window = 10,\n",
    "    epochs = 20,\n",
    ")\n",
    " \n",
    "# Finding most similar words\n",
    "print(\"3 words similar to car\")\n",
    "words = model.wv.most_similar(\"car\", topn=3)\n",
    "for word in words:\n",
    "    print(word)\n",
    "print()\n",
    " \n",
    "#Visualizing data\n",
    "words = [\"france\", \"germany\", \"india\", \"truck\", \"car\",\"road\", \"teacher\", \"student\"]\n",
    " \n",
    "X = model.wv[words]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    " \n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity of two n-dimensional vectors $\\mathbf{w_1}$ and $\\mathbf{w_2}$ is defined as:\n",
    "$$\\cos(\\mathbf{w_1},\\mathbf{w_2})=\\frac{\\mathbf{w_1} \\cdot \\mathbf{w_2}}{\\|\\mathbf{w_1}\\|\\|\\mathbf{w_2}\\|}=\\frac{\\sum_{i=1}^{n} {\\mathbf{w_{1_i}}} {\\mathbf{w_{2_i}}}}{\\sqrt{\\sum_{i=1}^{n} {{\\mathbf{w_{1_i}}}^{2}}} \\sqrt{\\sum_{i=1}^{n} {\\mathbf{w_{2_i}}}^{2}}}$$\n",
    "<img src=\"./cosine-similarity.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\\ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-trained word2vec embeddings from gensim\n",
    "# reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 similar words to BMW:\n",
      "('Audi', 0.7932199835777283)\n",
      "('Mercedes_Benz', 0.7683466672897339)\n",
      "('Porsche', 0.7272197604179382)\n",
      "('Mercedes', 0.7078384160995483)\n",
      "('Volkswagen', 0.6959410905838013)\n",
      "\n",
      "3 similar words to China:\n",
      "('Chinese', 0.7678080797195435)\n",
      "('Beijing', 0.7648460865020752)\n",
      "('Taiwan', 0.7081157565116882)\n",
      "\n",
      "Cosine similarity between fight and battle: 0.7021284\n",
      "\n",
      "Cosine similarity between China and book: 0.055054046\n"
     ]
    }
   ],
   "source": [
    "# Finding words similar to BMW\n",
    "print(\"5 similar words to BMW:\")\n",
    "words = google_news_vectors.most_similar(\"BMW\", topn=5)\n",
    "for word in words:\n",
    "    print(word)\n",
    "print()\n",
    " \n",
    "# Finding words similar to Love\n",
    "print(\"3 similar words to China:\")\n",
    "words = google_news_vectors.most_similar(\"China\", topn=3)\n",
    "for word in words:\n",
    "    print(word)\n",
    "print()\n",
    " \n",
    "# Finding cosine similarity between fight and battle\n",
    "cosine = google_news_vectors.similarity(\"fight\", \"battle\")\n",
    "print(\"Cosine similarity between fight and battle:\", cosine)\n",
    "print()\n",
    " \n",
    "# Finding cosine similarity between fight and love\n",
    "cosine = google_news_vectors.similarity(\"China\", \"book\")\n",
    "print(\"Cosine similarity between China and book:\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dissimilarity measure\n",
    "So I define my dissimilarity measure as $$\\text {dissim}(\\mathbf{w_1},\\mathbf{w_2})=\\left(\\frac{1}{\\cos(\\mathbf{w_1},\\mathbf{w_2})}-1\\right)||\\mathbf{w_1}-\\mathbf{w_2}||_2^2$$ We know that cosine similarity values range between -1 and 1, where -1 is indicating two word vectors are on opposite direction and 1 is they pointing at the same direction, but we also need to take their euclidean distance into account because two word can be dissimilar even if their vectors are pointing into same direction. In this metric, the furthur this dissimilarity measure to zero, the more dissimilar the two words are. 0 means they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilarity between fight and battle: 0.92319846\n",
      "Dissimilarity between fight and love: 61.747517\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Finding dissimilarity between fight and battle\n",
    "cosine = google_news_vectors.similarity(\"fight\", \"battle\")\n",
    "euclidean = np.linalg.norm(google_news_vectors['fight'] - google_news_vectors['battle'])\n",
    "print(\"Dissimilarity between fight and battle:\", euclidean/(cosine)-euclidean)\n",
    " \n",
    "# Finding dissimilarity between fight and love\n",
    "cosine = google_news_vectors.similarity(\"China\", \"book\")\n",
    "euclidean = np.linalg.norm(google_news_vectors['China'] - google_news_vectors['book'])\n",
    "print(\"Dissimilarity between fight and love:\", euclidean/(cosine)-euclidean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
